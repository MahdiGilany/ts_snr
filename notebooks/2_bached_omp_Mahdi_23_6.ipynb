{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from sklearn.datasets import make_sparse_coded_signal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "# from test_omp import omp_naive\n",
    "# from test import *  # FIXME: better name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 672]),\n",
       " torch.Size([1, 672, 1000]),\n",
       " torch.Size([1000]),\n",
       " True,\n",
       " True,\n",
       " True)"
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_0, X_0, w_0 = make_sparse_coded_signal(\n",
    "    n_samples=1,\n",
    "    n_components=1000,\n",
    "    n_features=96,\n",
    "    n_nonzero_coefs=10,\n",
    "    random_state=0)\n",
    "\n",
    "requires_grad = True\n",
    "y = torch.tensor(y_0, dtype=torch.float32, requires_grad=requires_grad).reshape(y_0.shape[0], -1).T.detach()\n",
    "X = torch.tensor(X_0, dtype=torch.float32, requires_grad=requires_grad).reshape(1, X_0.shape[0], -1).detach()\n",
    "w = torch.tensor(w_0, dtype=torch.float32, requires_grad=requires_grad).T.detach()\n",
    "X.requires_grad = True\n",
    "y.requires_grad = True\n",
    "w.requires_grad = True\n",
    "\n",
    "y.shape, X.shape, w.shape, y.is_leaf, X.is_leaf, w.is_leaf\n",
    "# y_hat = X @ w\n",
    "# ((y_hat-y)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdigilany/anaconda3/envs/borealis/lib/python3.9/site-packages/sklearn/datasets/_samples_generator.py:1335: FutureWarning: The default value of data_transposed will change from True to False in version 1.3\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.77817141, -2.08033674, -2.27265899, -0.62209651,  1.55394908,\n",
       "         1.43806502, -0.94548482, -0.48103745,  0.98482889, -0.71483167]),\n",
       " array([-0.77817141, -2.08033674, -2.27265899, -0.62209651,  1.55394908,\n",
       "         1.43806502, -0.94548482, -0.48103745,  0.98482889, -0.71483167]))"
      ]
     },
     "execution_count": 1010,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "n_nonzero_coefs=10\n",
    "y_, X_, w_ = make_sparse_coded_signal(\n",
    "    n_samples=1,\n",
    "    n_components=1000,\n",
    "    n_features=96,\n",
    "    n_nonzero_coefs=n_nonzero_coefs,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "omp_ = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\n",
    "reg = omp_.fit(X_, y_)\n",
    "omp_.coef_[omp_.coef_ != 0][:15], w_[w_ != 0][:15]\n",
    "# reg.score(X_0, y_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1351e-07)"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "dictionary_noise = (torch.rand_like(X.detach())*(X.std().detach()*.00000005))\n",
    "X_hat = X.detach() + dictionary_noise\n",
    "X_hat.requires_grad = True\n",
    "dictionary_noise.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Orthogonal Matching pursuit algorithm\n",
    "'''\n",
    "X_hat = X_hat/(X_hat.norm(dim=1, keepdim=True) + 1e-30)\n",
    "# X_hat = X\n",
    "dict = X_hat[0, ...] # consider cloning the tensor\n",
    "\n",
    "chunk_length, n_atoms = dict.shape\n",
    "batch_sz, chunk_length = y.shape\n",
    "n_nonzero_coefs = 100\n",
    "tau = 1e-3\n",
    "hard = True\n",
    "hard_mode = 0\n",
    "reg_=1e-10\n",
    "\n",
    "# DTD = dict.T @ dict\n",
    "\n",
    "residuals = y.clone().detach() # (batch_sz, chunk_length)\n",
    "residuals.requires_grad = True\n",
    "# max_score_indices = y.new_zeros((batch_sz, n_nonzero_coefs, n_atoms), dtype=X.dtype, device=X.device)\n",
    "max_score_indices = []\n",
    "sum_collector = torch.zeros((batch_sz, n_atoms), dtype=X.dtype, device=X.device)\n",
    "detached_indices = np.zeros((batch_sz, n_nonzero_coefs), dtype=np.int64) # (batch_sz, n_nonzero_coefs)\n",
    "\n",
    "\n",
    "tolerance = True\n",
    "# Control stop interation with norm thresh or sparsity\n",
    "for i in range(n_nonzero_coefs):\n",
    "    # Compute the score of each atoms\n",
    "    projections = dict.T @ residuals[:, :, None] # (batch_sz, n_atoms, 1)\n",
    "\n",
    "    detached_indices[:, i] = projections.abs().squeeze(-1).argmax(-1).detach().cpu().numpy()\n",
    "    soft_score_indices = (projections/tau).abs().squeeze(-1).softmax(-1) # (batch_sz, n_atoms)\n",
    "    \n",
    "    if hard:\n",
    "        # copied and modified from https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "        # Straight through.\n",
    "        index = soft_score_indices.max(-1, keepdim=True)[1]\n",
    "        hard_score_indices = torch.zeros_like(soft_score_indices).scatter_(-1, index, 1.0)\n",
    "        ret = hard_score_indices - soft_score_indices.detach() + soft_score_indices   \n",
    "        # max_score_indices[:, i] = ret\n",
    "        if hard_mode == 0:\n",
    "            max_score_indices.append(ret[:, None, :]) # (batch_sz, i+1, n_atoms), it is list on dimension of i+1\n",
    "            \n",
    "            # update selected_D torch.cat(max_score_indices, dim=1)\n",
    "            ## first indexing then multiplication with max_score_indices\n",
    "            ind_0 = torch.arange(batch_sz)[:, None, None]\n",
    "            ind_1 = torch.arange(chunk_length)[None, :, None]\n",
    "            _selected_D = X_hat[ind_0, ind_1, detached_indices[:, None, :i+1]] # (batch_sz, chunk_length, i+1)\n",
    "            ind_0 = torch.arange(batch_sz)[:, None]\n",
    "            ind_1 = torch.arange(i+1)[None, :]\n",
    "            _selected_max_score_indices = torch.cat(max_score_indices, dim=1)[ind_0, ind_1, detached_indices[:, :i+1]]\n",
    "            selected_D = _selected_D * _selected_max_score_indices[:, None, :] # (batch_sz, chunk_length, i+1)\n",
    "            ## first multiplication then indexing with max_score_indices \n",
    "            # _selected_D = X[:, None, ...] * torch.cat(max_score_indices, dim=1)[:, :i+1, None, :] # max_score_indices[:, :i+1, None, :] # (batch_sz, i+1, chunk_length, n_atoms)\n",
    "            # _selected_D = _selected_D.permute(0, 2, 1, 3) # (batch_sz, chunk_length, i+1, n_atoms)\n",
    "            # ind_0 = torch.arange(batch_sz)[:, None, None]\n",
    "            # ind_1 = torch.arange(chunk_length)[None, :, None]\n",
    "            # ind_2 = torch.arange(i+1)[None, None, :]\n",
    "            # selected_D = _selected_D[ind_0, ind_1, ind_2, detached_indices[:, None, :i+1]] # (batch_sz, chunk_length, i+1)\n",
    "            \n",
    "            # calculate selected_DTy\n",
    "            selected_DTy = selected_D.permute(0, 2, 1) @ y[:, :, None] # (batch_sz, i+1, 1)\n",
    "\n",
    "            # calculate selected_DTD\n",
    "            selected_DTD = selected_D.permute(0, 2, 1) @ selected_D # (batch_sz, i+1, i+1)\n",
    "\n",
    "            # find W = (selected_DTD)^-1 @ selected_DTy\n",
    "            selected_DTD.diagonal(dim1=-2, dim2=-1).add_(reg_) # TODO: add multipath OMP\n",
    "            nonzero_W = torch.linalg.solve(selected_DTD, selected_DTy) # (batch_sz, i+1, 1)\n",
    "\n",
    "            # finally get residuals r=y-Wx\n",
    "            residuals = y - (selected_D @ nonzero_W).squeeze(-1) # (batch_sz, chunk_length)\n",
    "        else:\n",
    "            sum_collector = sum_collector + ret\n",
    "            selected_D = X_hat * sum_collector[:, None, :] # (batch_sz, chunk_length, n_atoms)\n",
    "            \n",
    "            # calculate selected_DTy\n",
    "            selected_DTy = selected_D.permute(0, 2, 1) @ y[:, :, None] # (batch_sz, n_atoms, 1)\n",
    "\n",
    "            # calculate selected_DTD\n",
    "            selected_DTD = selected_D.permute(0, 2, 1) @ selected_D # (batch_sz, n_atoms, n_atoms)\n",
    "\n",
    "            # find W = (selected_DTD)^-1 @ selected_DTy\n",
    "            selected_DTD.diagonal(dim1=-2, dim2=-1).add_(reg_) # TODO: add multipath OMP\n",
    "            nonzero_W = torch.linalg.solve(selected_DTD, selected_DTy) # (batch_sz, n_atoms, 1)\n",
    "\n",
    "            # finally get residuals r=y-Wx\n",
    "            residuals = y - (selected_D @ nonzero_W).squeeze(-1) # (batch_sz, chunk_length)\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        # raise NotImplementedError\n",
    "        sum_collector = sum_collector + soft_score_indices\n",
    "        selected_D = X_hat * sum_collector[:, None, :] # (batch_sz, chunk_length, n_atoms)\n",
    "        \n",
    "        # calculate selected_DTy\n",
    "        selected_DTy = selected_D.permute(0, 2, 1) @ y[:, :, None] # (batch_sz, n_atoms, 1)\n",
    "\n",
    "        # calculate selected_DTD\n",
    "        selected_DTD = selected_D.permute(0, 2, 1) @ selected_D # (batch_sz, n_atoms, n_atoms)\n",
    "\n",
    "        # find W = (selected_DTD)^-1 @ selected_DTy\n",
    "        selected_DTD.diagonal(dim1=-2, dim2=-1).add_(reg_) # TODO: add multipath OMP\n",
    "        nonzero_W = torch.linalg.solve(selected_DTD, selected_DTy) # (batch_sz, n_atoms, 1)\n",
    "\n",
    "        # finally get residuals r=y-Wx\n",
    "        residuals = y - (selected_D @ nonzero_W).squeeze(-1) # (batch_sz, chunk_length)\n",
    "\n",
    "if hard:\n",
    "    if hard_mode == 0:\n",
    "        W = torch.zeros(batch_sz, n_atoms, dtype=selected_D.dtype, device=selected_D.device)\n",
    "        W[torch.arange(batch_sz)[:, None], detached_indices] = nonzero_W.squeeze(-1)\n",
    "    else:\n",
    "        W = nonzero_W.squeeze(-1)\n",
    "else:\n",
    "    W = nonzero_W.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8077e-06, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 804,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = X_hat @ W[..., None]\n",
    "loss = ((y_hat.squeeze(-1)-y)).norm()\n",
    "loss\n",
    "# loss.backward(), loss, detached_indices[0,:10], X.grad, X_hat.grad[0, :, 78][:10], X_hat.grad[0, :, 230][:10], X_hat.grad.abs().sum(), X_hat.grad[0, :, 78].abs().sum(), X_hat.grad[0, :, 230].abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.2283,  0.7654,  0.4563, -0.5398,  0.0076, -0.0651,  0.9550,  0.4489,\n",
       "          0.5807,  2.3090], grad_fn=<SliceBackward0>),\n",
       " tensor([ 1.2283,  0.7654,  0.4563, -0.5398,  0.0076, -0.0651,  0.9550,  0.4489,\n",
       "          0.5807,  2.3090], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[W!=0][:10], w[w!=0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_sums = []\n",
    "for i in range(len(detached_indices[0])):\n",
    "    abs_sums.append(X_hat.grad[0, :, detached_indices[0, i]].abs().sum())\n",
    "plt.plot(abs_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_loss = W.sum() + 100\n",
    "fake_loss = (nonzero_W**5).sum() + 100\n",
    "fake_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.grad, y.grad, w.grad, X.is_leaf, soft_score_indices.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alex idea for OMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdigilany/anaconda3/envs/borealis/lib/python3.9/site-packages/sklearn/datasets/_samples_generator.py:1335: FutureWarning: The default value of data_transposed will change from True to False in version 1.3\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 96]),\n",
       " torch.Size([1, 96, 1000]),\n",
       " torch.Size([1000]),\n",
       " True,\n",
       " True,\n",
       " True)"
      ]
     },
     "execution_count": 1011,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 10\n",
    "y_0, X_0, w_0 = make_sparse_coded_signal(\n",
    "    n_samples=1,\n",
    "    n_components=1000,\n",
    "    n_features=96,\n",
    "    n_nonzero_coefs=n_components,\n",
    "    random_state=0)\n",
    "\n",
    "requires_grad = True\n",
    "y = torch.tensor(y_0, dtype=torch.float32, requires_grad=requires_grad).reshape(y_0.shape[0], -1).T.detach()\n",
    "X = torch.tensor(X_0, dtype=torch.float32, requires_grad=requires_grad).reshape(1, X_0.shape[0], -1).detach()\n",
    "w = torch.tensor(w_0, dtype=torch.float32, requires_grad=requires_grad).T.detach()\n",
    "X.requires_grad = True\n",
    "y.requires_grad = True\n",
    "w.requires_grad = True\n",
    "\n",
    "y.shape, X.shape, w.shape, y.is_leaf, X.is_leaf, w.is_leaf\n",
    "# y_hat = X @ w\n",
    "# ((y_hat-y)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0164)"
      ]
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "dictionary_noise = (torch.rand_like(X.detach())*(X.std().detach()*.0009))\n",
    "X_hat = X.detach() + dictionary_noise\n",
    "X_hat.requires_grad = True\n",
    "dictionary_noise.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Orthogonal Matching pursuit algorithm\n",
    "'''\n",
    "# X_hat = X_hat/(X_hat.norm(dim=1, keepdim=True) + 1e-30)\n",
    "X_hat = X\n",
    "dict = X_hat[0, ...] # consider cloning the tensor\n",
    "\n",
    "\n",
    "chunk_length, n_atoms = dict.shape\n",
    "batch_sz, chunk_length = y.shape\n",
    "n_nonzero_coefs = n_components\n",
    "tau = 1e-30\n",
    "hard = True\n",
    "reg_ = 1e-15\n",
    "\n",
    "# DTD = dict.T @ dict\n",
    "\n",
    "residuals = y.clone().detach() # (batch_sz, chunk_length)\n",
    "residuals.requires_grad = True\n",
    "\n",
    "max_score_indices = []\n",
    "detached_indices = np.zeros((batch_sz, n_nonzero_coefs), dtype=np.int64) # (batch_sz, n_nonzero_coefs)\n",
    "# max_score_indices = y.new_zeros((batch_sz, n_nonzero_coefs, n_atoms), dtype=X.dtype, device=X.device)\n",
    "# sum_collector = torch.zeros((batch_sz, n_atoms), dtype=X.dtype, device=X.device)\n",
    "\n",
    "\n",
    "tolerance = True\n",
    "# Control stop interation with norm thresh or sparsity\n",
    "for i in range(n_nonzero_coefs):\n",
    "    # Compute the score of each atoms\n",
    "    projections = dict.T @ residuals[:, :, None] # (batch_sz, n_atoms, 1)\n",
    "    soft_score_indices = (projections/tau).abs().squeeze(-1).softmax(-1) # (batch_sz, n_atoms)\n",
    "    detached_indices[:, i] = projections.abs().squeeze(-1).argmax(-1).detach().cpu().numpy() # (batch_sz, ) for final W\n",
    "\n",
    "    if hard:\n",
    "        # copied and modified from https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "        # Straight through.\n",
    "        index = soft_score_indices.max(-1, keepdim=True)[1]\n",
    "        hard_score_indices = torch.zeros_like(soft_score_indices).scatter_(-1, index, 1.0)\n",
    "        ret = hard_score_indices - soft_score_indices.detach() + soft_score_indices # (batch_sz, n_atoms)  \n",
    "\n",
    "        max_score_indices.append(ret[:, :, None]) # list[(batch_sz, n_atoms)] * i+1\n",
    "        \n",
    "        # update selected_D\n",
    "        selected_D = X_hat @ torch.cat(max_score_indices, dim=2) # (batch_sz, chunk_length, i+1)\n",
    "        \n",
    "        # calculate selected_DTy\n",
    "        selected_DTy = selected_D.permute(0, 2, 1) @ y[:, :, None] # (batch_sz, i+1, 1)\n",
    "\n",
    "        # calculate selected_DTD\n",
    "        selected_DTD = selected_D.permute(0, 2, 1) @ selected_D # (batch_sz, i+1, i+1)\n",
    "\n",
    "        # find W = (selected_DTD)^-1 @ selected_DTy\n",
    "        selected_DTD.diagonal(dim1=-2, dim2=-1).add_(reg_) # TODO: add multipath OMP\n",
    "        # print(i)\n",
    "        nonzero_W = torch.linalg.solve(selected_DTD, selected_DTy) # (batch_sz, i+1, 1)\n",
    "\n",
    "        # finally get residuals r=y-Wx\n",
    "        residuals = y.detach() - (selected_D @ nonzero_W).squeeze(-1) # (batch_sz, chunk_length)                    \n",
    "            \n",
    "    else:\n",
    "        max_score_indices.append(soft_score_indices[:, :, None]) # list[(batch_sz, n_atoms)] * i+1\n",
    "        \n",
    "        # update selected_D\n",
    "        selected_D = X_hat @ torch.cat(max_score_indices, dim=2) # (batch_sz, chunk_length, i+1)\n",
    "        \n",
    "        # calculate selected_DTy\n",
    "        selected_DTy = selected_D.permute(0, 2, 1) @ y[:, :, None] # (batch_sz, i+1, 1)\n",
    "\n",
    "        # calculate selected_DTD\n",
    "        selected_DTD = selected_D.permute(0, 2, 1) @ selected_D # (batch_sz, i+1, i+1)\n",
    "\n",
    "        # find W = (selected_DTD)^-1 @ selected_DTy\n",
    "        # selected_DTD.diagonal(dim1=-2, dim2=-1).add_(reg_) # TODO: add multipath OMP\n",
    "        \n",
    "        nonzero_W = torch.linalg.solve(selected_DTD, selected_DTy) # (batch_sz, i+1, 1)\n",
    "\n",
    "        # finally get residuals r=y-Wx\n",
    "        residuals = y.detach() - (selected_D @ nonzero_W).squeeze(-1) # (batch_sz, chunk_length)        \n",
    "\n",
    "\n",
    "W = torch.zeros(batch_sz, n_atoms, dtype=selected_D.dtype, device=selected_D.device,)\n",
    "W[torch.arange(batch_sz)[:, None], detached_indices] = nonzero_W.squeeze(-1)\n",
    "# aaa=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor(4.8279e-07, grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(49.4592),\n",
       " array([183, 466, 559, 587, 934, 636, 474, 155, 973, 855]),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 1014,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = X_hat @ W[..., None]\n",
    "loss = ((y_hat.squeeze(-1)-y)).norm()\n",
    "loss\n",
    "loss.backward(), loss, X_hat.grad.abs().sum(), detached_indices[0,:10], X.grad, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor(0.),\n",
       " tensor(0.))"
      ]
     },
     "execution_count": 1015,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hat.grad[0, :, 78][:10], X_hat.grad[0, :, 230][:10], X_hat.grad[0, :, 78].abs().sum(), X_hat.grad[0, :, 230].abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.7782, -2.0803, -2.2727, -0.6221,  1.5539,  1.4381, -0.9455, -0.4810,\n",
       "          0.9848, -0.7148], grad_fn=<SliceBackward0>),\n",
       " tensor([-0.7782, -2.0803, -2.2727, -0.6221,  1.5539,  1.4381, -0.9455, -0.4810,\n",
       "          0.9848, -0.7148], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 1016,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[W!=0][:15], w[w!=0][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fefd849a6d0>]"
      ]
     },
     "execution_count": 1017,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abs_sums = []\n",
    "for i in range(len(detached_indices[0])):\n",
    "    abs_sums.append(X_hat.grad[0, :, detached_indices[0, i]].abs().sum())\n",
    "plt.plot(abs_sums[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_loss = W.sum() + 100\n",
    "fake_loss = (nonzero_W**1).sum() + 0\n",
    "fake_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.grad[0, :, 78], detached_indices,X.grad.abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor(2., requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, True, True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(2.0, requires_grad = True)\n",
    "print(\"x:\", x)\n",
    "y = x**2\n",
    "y.backward()\n",
    "x.grad\n",
    "y.is_leaf, x.is_leaf, x.grad.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5408,  1.2117,  1.3392, -0.2476,  0.8874, -0.9175,  0.4356,  0.6046,\n",
       "        -2.0075, -0.3002, -0.5613,  1.1176,  1.4062, -0.0574,  0.1614, -0.6855,\n",
       "         0.2678,  0.8091, -0.9437, -1.0636, -1.8954,  0.7429, -0.0113, -0.7047,\n",
       "         1.5529,  0.7159,  0.2365,  0.0320,  0.6710, -0.2902,  1.4943,  1.2897,\n",
       "        -0.8026, -1.0412, -0.2379, -0.4289,  0.3764, -1.0462, -1.1264,  0.4701,\n",
       "         1.4027, -0.2258,  0.0994, -0.0367,  0.2440,  1.1910,  0.3706,  0.5264,\n",
       "         1.4489, -0.4281,  0.3012,  1.2043, -0.5630,  1.0285, -0.2981, -1.8573,\n",
       "        -1.2280, -1.0510, -0.8269, -1.0729,  0.1356,  0.7133,  0.0422,  1.2057,\n",
       "         1.5344, -1.3419, -0.0380,  0.7402, -1.3527, -0.2217,  2.4217, -0.6209,\n",
       "        -0.2137,  0.3353,  0.1102, -0.6201, -1.6132,  0.1191,  0.1394, -0.0561,\n",
       "        -0.4938,  2.1766, -1.3532,  0.2044, -1.6174, -0.8904,  1.3027,  0.7292,\n",
       "         0.3150,  1.3376,  0.1009,  0.5461, -2.3974,  1.6264, -0.1896, -0.7224,\n",
       "        -0.7440,  0.9898, -0.4637, -0.3934,  0.0614,  0.7366,  1.2403, -0.2427,\n",
       "        -0.3837, -1.2817, -0.3566, -0.7473, -0.9672,  0.1831, -0.2627, -1.2858,\n",
       "         0.2873,  1.5062, -0.0683, -1.1253,  0.3272,  2.0892,  1.5431,  0.3289,\n",
       "         1.0886,  0.4305,  0.4469,  0.2880,  1.1550, -0.4048,  0.4201,  0.1696,\n",
       "        -1.1198,  0.4139,  0.9704, -0.7813, -2.6156,  0.8165,  1.4397, -0.0633,\n",
       "        -0.2116, -0.5279, -0.2328, -1.1809, -0.4549,  0.2210,  0.3838,  0.4902,\n",
       "         0.5891, -1.0232,  0.5438, -1.8012,  0.8603,  0.7119,  0.2003,  0.5365,\n",
       "        -1.8237, -0.6780,  1.1432,  0.3813,  0.7832, -1.6020, -1.0148, -1.1166,\n",
       "         2.3496,  0.5338, -0.2390,  0.6100,  0.1470,  1.1454, -0.1552, -1.1014,\n",
       "        -0.8970,  0.2117, -1.4811,  0.1700,  1.7829,  2.0669,  1.5193, -1.5507,\n",
       "         0.1252, -2.0240,  0.0955,  0.7245,  0.8765, -0.1101,  1.8965, -0.6627,\n",
       "        -1.3161,  0.2009, -0.8494, -1.0368, -1.0483, -0.9172,  0.5173, -0.2099,\n",
       "         0.3820, -0.3885,  1.0298,  1.0658, -0.3150,  0.9874, -1.8882, -0.5106],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[w!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(max_score_indices, dim=1)[0, :][torch.cat(max_score_indices, dim=1)[0, :]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3781,  0.5507,  0.0411,  0.4116, -0.3544, -0.3975,  0.3310, -0.5897,\n",
       "         0.5411,  0.4769, -0.1108,  0.6286, -0.5244,  0.0532, -0.6963, -0.4543,\n",
       "        -0.5881, -0.2190,  0.0703, -0.4413,  0.3593,  0.2277, -0.4663, -0.5157,\n",
       "         0.0351,  0.0877, -0.1503, -0.5425,  0.0225,  0.1120, -0.4264,  0.5205,\n",
       "        -0.4977,  0.3502, -0.0114, -0.3009,  0.2730,  0.0585,  0.5792,  0.1418,\n",
       "        -0.2939,  0.3730, -0.2957,  0.3881, -0.1518,  0.5543, -0.0378,  0.0041,\n",
       "        -0.3575,  0.2728, -0.1623, -0.3832, -0.5734, -0.5295, -0.1485, -0.0939,\n",
       "        -0.2373,  0.4941, -0.3694,  0.6005, -0.5624, -0.3042,  0.4301,  0.3673,\n",
       "        -0.4097, -0.4645,  0.2106,  0.5288, -0.4540, -0.4228, -0.4433,  0.2331,\n",
       "        -0.4318, -0.4201,  0.1485, -0.6147,  0.5219,  0.1050,  0.4204, -0.1920,\n",
       "        -0.1441,  0.4040,  0.4054,  0.4164, -0.7370, -0.4144,  0.0559, -0.1843,\n",
       "        -0.4349, -0.0398,  0.4193,  0.5104, -0.1027,  0.4634, -0.2689, -0.3532,\n",
       "         0.0082, -0.4478,  0.0118,  0.4786,  0.3983, -0.4113,  0.5193,  0.4283,\n",
       "         0.0878,  0.0413, -0.1211,  0.3693,  0.4060, -0.3742, -0.2221, -0.5386,\n",
       "         0.2877,  0.1001,  0.3205, -0.3527, -0.3723,  0.1723, -0.6162,  0.3278,\n",
       "         0.4245,  0.2520, -0.4410,  0.3365, -0.2775,  0.4979, -0.1065,  0.5025,\n",
       "         0.3243, -0.3457,  0.0380, -0.4846, -0.1459,  0.4385,  0.4480,  0.4802,\n",
       "         0.2108,  0.4071,  0.3197,  0.5530, -0.2913, -0.1851, -0.0470,  0.4192,\n",
       "         0.5848, -0.2310, -0.4926,  0.4492,  0.2995,  0.3027,  0.4144,  0.2193,\n",
       "         0.1217,  0.3576,  0.6525, -0.3780,  0.3946, -0.0303, -0.7048, -0.2594,\n",
       "         0.3073,  0.4010,  0.1378,  0.3512, -0.6655,  0.1737, -0.3685, -0.0297,\n",
       "         0.5371,  0.2749, -0.1555, -0.2475,  0.1611], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[W!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid_sample differentiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[5., 6.]]]], grad_fn=<GridSampler2DBackward0>) torch.Size([1, 1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "src = torch.arange(25, dtype=torch.float).reshape(1, 1, 5, 5).requires_grad_()  # 1 x 1 x 6 x 5 with 0 ... 30\n",
    "indices = torch.tensor([[-1, 0],[1, 1]], dtype=torch.float).reshape(1, 1, -1, 2)  # 1 x 1 x 2 x 2\n",
    "output = F.grid_sample(src, indices)\n",
    "print(output, output.shape)  # tensor([[[[  0.,  12.]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.,  9.],\n",
       "          [10., 11., 12., 13., 14.],\n",
       "          [15., 16., 17., 18., 19.],\n",
       "          [20., 21., 22., 23., 24.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.5000]]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]], grad_fn=<DivBackward0>)\n",
      "tensor([-4.5000,  9.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = Variable(torch.arange(9, dtype=torch.float).reshape(-1,3), requires_grad=True)\n",
    "idx = Variable(torch.FloatTensor([0,1]), requires_grad=True)\n",
    "\n",
    "\n",
    "i0 = idx.floor().detach()\n",
    "i1 = i0 + 1\n",
    "i_1 = i0 - 1\n",
    "\n",
    "y0 = x[i0.long(), :]\n",
    "y1 = x[i1.long(), :]\n",
    "y_1 = x[i_1.long(), :]\n",
    "\n",
    "Wa = (i1 - idx).unsqueeze(1).expand_as(y0)\n",
    "Wb = (idx - i0).unsqueeze(1).expand_as(y1)\n",
    "Wa2 = (idx - i_1).unsqueeze(1).expand_as(y_1)\n",
    "Wb2 = (i0 - idx).unsqueeze(1).expand_as(y_1)\n",
    "\n",
    "out = (Wa * y0 + Wb * y1 + Wa2 * y0 + Wb2 * y_1)/2\n",
    "\n",
    "print(out)\n",
    "out.sum().backward()\n",
    "print(idx.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]], grad_fn=<ExpandBackward0>),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]], grad_fn=<ExpandBackward0>))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0, y1\n",
    "Wa2, Wb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 7., 8.],\n",
       "        [0., 1., 2.]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.],\n",
       "        [6., 7., 8.]], requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 2.],\n",
       "          [3., 4., 5.],\n",
       "          [6., 7., 8.]]]], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = x.reshape(1,1,3,3)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdigilany/anaconda3/envs/borealis/lib/python3.9/site-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5000]]]], grad_fn=<GridSampler2DBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grid = torch.tensor([0, -1.]).reshape(1,1,1,2).float()\n",
    "F.grid_sample(xx, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.,  9.],\n",
      "          [10., 11., 12., 13., 14.],\n",
      "          [15., 16., 17., 18., 19.],\n",
      "          [20., 21., 22., 23., 24.]]]])\n",
      "tensor([[[[ 0.0000,  0.4444,  0.8889,  1.3333,  1.7778,  2.2222,  2.6667,\n",
      "            3.1111,  3.5556,  4.0000],\n",
      "          [ 2.2222,  2.6667,  3.1111,  3.5556,  4.0000,  4.4444,  4.8889,\n",
      "            5.3333,  5.7778,  6.2222],\n",
      "          [ 4.4444,  4.8889,  5.3333,  5.7778,  6.2222,  6.6667,  7.1111,\n",
      "            7.5556,  8.0000,  8.4444],\n",
      "          [ 6.6667,  7.1111,  7.5556,  8.0000,  8.4444,  8.8889,  9.3333,\n",
      "            9.7778, 10.2222, 10.6667],\n",
      "          [ 8.8889,  9.3333,  9.7778, 10.2222, 10.6667, 11.1111, 11.5556,\n",
      "           12.0000, 12.4444, 12.8889],\n",
      "          [11.1111, 11.5556, 12.0000, 12.4444, 12.8889, 13.3333, 13.7778,\n",
      "           14.2222, 14.6667, 15.1111],\n",
      "          [13.3333, 13.7778, 14.2222, 14.6667, 15.1111, 15.5556, 16.0000,\n",
      "           16.4444, 16.8889, 17.3333],\n",
      "          [15.5556, 16.0000, 16.4444, 16.8889, 17.3333, 17.7778, 18.2222,\n",
      "           18.6667, 19.1111, 19.5556],\n",
      "          [17.7778, 18.2222, 18.6667, 19.1111, 19.5556, 20.0000, 20.4444,\n",
      "           20.8889, 21.3333, 21.7778],\n",
      "          [20.0000, 20.4444, 20.8889, 21.3333, 21.7778, 22.2222, 22.6667,\n",
      "           23.1111, 23.5556, 24.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.arange(5*5).view(1, 1, 5, 5).float()\n",
    "print(input)\n",
    "\n",
    "# Create grid to upsample input\n",
    "d = torch.linspace(-1, 1, 10)\n",
    "meshx, meshy = torch.meshgrid((d, d))\n",
    "grid = torch.stack((meshy, meshx), 2)\n",
    "grid = grid.unsqueeze(0) # add batch dim\n",
    "\n",
    "output = torch.nn.functional.grid_sample(input, grid, align_corners=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.reshape(1,5,1,5)[0,0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1, 1])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = torch.tensor([-1, 0]).reshape(1,1,1,2).float()\n",
    "F.grid_sample(input.reshape(1,5,1,5), grid, align_corners=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Orthogonal Matching pursuit algorithm\n",
    "'''\n",
    "dict = X_hat[0, ...] # consider cloning the tensor\n",
    "\n",
    "chunk_length, n_atoms = dict.shape\n",
    "batch_sz, chunk_length = y.shape\n",
    "n_nonzero_coefs = 15\n",
    "tau = 1e-3\n",
    "hard = True\n",
    "\n",
    "# DTD = dict.T @ dict\n",
    "\n",
    "residuals = y.clone().detach() # (batch_sz, chunk_length)\n",
    "residuals.requires_grad = True\n",
    "# max_score_indices = y.new_zeros((batch_sz, n_nonzero_coefs, n_atoms), dtype=X.dtype, device=X.device)\n",
    "max_score_indices = []\n",
    "detached_indices = np.zeros((batch_sz, n_nonzero_coefs), dtype=np.int64) # (batch_sz, n_nonzero_coefs)\n",
    "\n",
    "\n",
    "tolerance = True\n",
    "# Control stop interation with norm thresh or sparsity\n",
    "for i in range(n_nonzero_coefs): \n",
    "    # Compute the score of each atoms\n",
    "    projections = dict.T @ residuals[:, :, None] # (batch_sz, n_atoms, 1)\n",
    "\n",
    "    detached_indices[:, i] = projections.abs().squeeze(-1).argmax(-1).detach().cpu().numpy()\n",
    "    soft_score_indices = (projections/tau).abs().squeeze(-1).softmax(-1) # (batch_sz, n_atoms)\n",
    "    if hard:\n",
    "        # copied and modified from https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "        # Straight through.\n",
    "        index = soft_score_indices.max(-1, keepdim=True)[1]\n",
    "        hard_score_indices = torch.zeros_like(soft_score_indices).scatter_(-1, index, 1.0)\n",
    "        ret = hard_score_indices - soft_score_indices.detach() + soft_score_indices   \n",
    "        # max_score_indices[:, i] = ret\n",
    "        max_score_indices.append(ret[:, None, :])\n",
    "    else:\n",
    "        max_score_indices.append(soft_score_indices[:, None, :])\n",
    "    \n",
    "    # update selected_D torch.cat(max_score_indices, dim=1)\n",
    "    _selected_D = X_hat[:, None, ...] * torch.cat(max_score_indices, dim=1)[:, :i+1, None, :] # max_score_indices[:, :i+1, None, :] # (batch_sz, i+1, chunk_length, n_atoms)\n",
    "    _selected_D = _selected_D.permute(0, 2, 1, 3) # (batch_sz, chunk_length, i+1, n_atoms)\n",
    "    ind_0 = torch.arange(batch_sz)[:, None, None]\n",
    "    ind_1 = torch.arange(chunk_length)[None, :, None]\n",
    "    ind_2 = torch.arange(i+1)[None, None, :]\n",
    "    selected_D = _selected_D[ind_0, ind_1, ind_2, detached_indices[:, None, :i+1]] # (batch_sz, chunk_length, i+1)\n",
    "    \n",
    "    # s_D = dict[torch.arange(chunk_length)[:, None], detached_indices[:, :i+1]] # (chunk_length, i+1)\n",
    "\n",
    "    # calculate selected_DTy\n",
    "    selected_DTy = selected_D.permute(0, 2, 1) @ y[:, :, None] # (batch_sz, i+1, 1)\n",
    "\n",
    "    # calculate selected_DTD\n",
    "    selected_DTD = selected_D.permute(0, 2, 1) @ selected_D # (batch_sz, i+1, i+1)\n",
    "\n",
    "    # find W = (selected_DTD)^-1 @ selected_DTy\n",
    "    # selected_DTD.diagonal(dim1=-2, dim2=-1).add_(1.) # TODO: add multipath OMP\n",
    "    nonzero_W = torch.linalg.solve(selected_DTD, selected_DTy) # (batch_sz, i+1, 1)\n",
    "\n",
    "    # finally get residuals r=y-Wx\n",
    "    residuals = y.detach() - (selected_D @ nonzero_W).squeeze(-1) # (batch_sz, chunk_length, 1)\n",
    "\n",
    "W = torch.zeros(batch_sz, n_atoms, dtype=selected_D.dtype, device=selected_D.device)\n",
    "W[torch.arange(batch_sz)[:, None], detached_indices] = nonzero_W.squeeze(-1)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "borealis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
