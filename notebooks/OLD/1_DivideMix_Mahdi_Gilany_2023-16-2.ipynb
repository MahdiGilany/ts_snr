{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mahdigilany/offline_codes/codes/TRUSnet\n"
     ]
    }
   ],
   "source": [
    "# import sys \n",
    "# sys.path.append('../')\n",
    "%cd ..\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.layers.losses.losses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfiguration\u001b[39;00m \u001b[39mimport\u001b[39;00m register_configs\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 8\u001b[0m register_configs()\n\u001b[1;32m     10\u001b[0m GlobalHydra\u001b[39m.\u001b[39minstance()\u001b[39m.\u001b[39mclear()\n\u001b[1;32m     11\u001b[0m initialize(config_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../configs\u001b[39m\u001b[39m\"\u001b[39m, version_base\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m1.1\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/offline_codes/codes/TRUSnet/src/configuration.py:24\u001b[0m, in \u001b[0;36mregister_configs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m cs\u001b[39m.\u001b[39mstore(\u001b[39m\"\u001b[39m\u001b[39mpatch_dm_sl_base\u001b[39m\u001b[39m\"\u001b[39m, ExactPatchDMConfigSL, group\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdatamodule\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m cs\u001b[39m.\u001b[39mstore(\u001b[39m\"\u001b[39m\u001b[39mcore_dm_mixing_base\u001b[39m\u001b[39m\"\u001b[39m, ExactCoreDMConfigWithMixing, group\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdatamodule\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlightning_modules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfigure_optimizer_mixin\u001b[39;00m \u001b[39mimport\u001b[39;00m OptimizerConfig\n\u001b[1;32m     26\u001b[0m cs\u001b[39m.\u001b[39mstore(\u001b[39m\"\u001b[39m\u001b[39moptim_defaults\u001b[39m\u001b[39m\"\u001b[39m, OptimizerConfig, group\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlightning_modules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mself_supervised\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvicreg\u001b[39;00m \u001b[39mimport\u001b[39;00m VICRegConfig\n",
      "File \u001b[0;32m~/offline_codes/codes/TRUSnet/src/lightning/lightning_modules/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mself_supervised\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvicreg\u001b[39;00m \u001b[39mimport\u001b[39;00m VICReg\n",
      "File \u001b[0;32m~/offline_codes/codes/TRUSnet/src/lightning/lightning_modules/self_supervised/vicreg.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, List, Optional, Sequence\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39momegaconf\u001b[39;00m \u001b[39mimport\u001b[39;00m MISSING, DictConfig\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mssl_base\u001b[39;00m \u001b[39mimport\u001b[39;00m SSLBase\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m FeatureExtractionProtocol\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistry\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistry\u001b[39;00m \u001b[39mimport\u001b[39;00m create_model\n",
      "File \u001b[0;32m~/offline_codes/codes/TRUSnet/src/lightning/lightning_modules/self_supervised/ssl_base.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningModule\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlosses\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlosses\u001b[39;00m \u001b[39mimport\u001b[39;00m vicreg_loss_func\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[1;32m     13\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSSLBase\u001b[39;00m(ConfigureOptimizersMixin, LightningModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.layers.losses.losses'"
     ]
    }
   ],
   "source": [
    "from hydra import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from src.configuration import register_configs\n",
    "from tqdm import tqdm\n",
    "\n",
    "register_configs()\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"../configs\", version_base=\"1.1\")\n",
    "\n",
    "dm_config = compose(\n",
    "    config_name=\"datamodule/ssl_tuffc.yaml\",\n",
    "    overrides=[\n",
    "        \"datamodule.splits.cohort_specifier=['UVA600', 'CRCEO428']\",\n",
    "        \"datamodule.splits.train_val_split_seed=0\",\n",
    "        \"datamodule.splits.test_as_val=True\",\n",
    "        \"datamodule.patch_view_config.needle_region_only=True\",\n",
    "        \"datamodule.patch_view_config.prostate_region_only=False\",\n",
    "        \"datamodule._target_='src.lightning.datamodules.exact_datamodule.PatchDataModuleForSemiSupervisedLearningDivideMix'\",\n",
    "        \"datamodule.loader_config.num_workers=0\",\n",
    "        \"datamodule.loader_config.batch_size=8\",\n",
    "\n",
    "    ]\n",
    "    )\n",
    "lightning_data_module = instantiate(dm_config)['datamodule']\n",
    "# print(lightning_data_module)\n",
    "lightning_data_module.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lightning.lightning_modules.evaluation_base import EvaluationBase \n",
    "from typing import Optional, List\n",
    "from src.modeling.optimizer_factory import OptimizerConfig\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchmetrics import Accuracy\n",
    "from contextlib import nullcontext\n",
    "\n",
    "class DivideMix(EvaluationBase):\n",
    "    \"\"\"\n",
    "    This class implements DivideMix, a semi-supervised learning method for prostate cancer detection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone1: DictConfig,\n",
    "        backbone2: DictConfig,\n",
    "        checkpoint: Optional[str] = None,\n",
    "        semi_sup: bool = True,\n",
    "        batch_size: int = 32,\n",
    "        epochs: int = 100,\n",
    "        num_classes: int = 2,\n",
    "        temperature: float = 0.1,\n",
    "        beta_alpha: float = 0.75,\n",
    "        GMM_prob_threshold: float = 0.5,\n",
    "        GMM_cycle: int = 3,\n",
    "        opt_cfg: OptimizerConfig = OptimizerConfig(),\n",
    "    ):\n",
    "\n",
    "        super().__init__(batch_size, epochs, opt_cfg=opt_cfg)\n",
    "\n",
    "        self.semi_sup = semi_sup\n",
    "        self.temperature = temperature\n",
    "        self.beta_alpha = beta_alpha\n",
    "        self.num_classes = num_classes\n",
    "        self.GMM_prob_threshold = GMM_prob_threshold\n",
    "        self.GMM_cycle = GMM_cycle\n",
    "        \n",
    "        # logging.getLogger(__name__).info(\n",
    "        #     f\"Instantiating model {backbone._target_} as backbone for finetuner\"\n",
    "        # )\n",
    "        finetune1 = instantiate(backbone1)\n",
    "        finetune2 = instantiate(backbone2)\n",
    "\n",
    "        # assert isinstance(\n",
    "        #     backbone, FeatureExtractionProtocol\n",
    "        # ), \"Finetuned model must support feature extraction\"\n",
    "        self.backbone1 = finetune1.backbone\n",
    "        self.backbone2 = finetune2.backbone\n",
    "\n",
    "        # self.linear_layer1 = torch.nn.Linear(self.backbone1.features_dim, num_classes)\n",
    "        # self.linear_layer2 = torch.nn.Linear(self.backbone2.features_dim, num_classes)\n",
    "        self.linear_layer1 = finetune1.linear_layer\n",
    "        self.linear_layer2 = finetune2.linear_layer\n",
    "\n",
    "        self._checkpoint_is_loaded = False\n",
    "\n",
    "        if checkpoint is not None:\n",
    "            self.load_from_pretraining_ckpt(checkpoint)\n",
    "\n",
    "        self.train_acc = Accuracy()\n",
    "\n",
    "        self.inferred_no_centers = 1\n",
    "\n",
    "        self.train_lossHistory1 = []\n",
    "        self.train_lossHistory2 = []\n",
    "        self.val_macroLoss_all_centers = []\n",
    "        self.test_macroLoss_all_centers = []\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def load_from_pretraining_ckpt(self, ckpt: str):\n",
    "        if isinstance(self.backbone1, LightningModule):\n",
    "            self.backbone1.load_from_checkpoint(ckpt)\n",
    "            self.backbone2.load_from_checkpoint(ckpt)\n",
    "\n",
    "        self._checkpoint_is_loaded = True\n",
    "\n",
    "    @property\n",
    "    def learnable_parameters(self) -> List[dict]:\n",
    "        \"\"\"Adds projector parameters to the parent's learnable parameters.\n",
    "\n",
    "        Returns:\n",
    "            List[dict]: list of learnable parameters.\n",
    "        \"\"\"\n",
    "        parameters = [\n",
    "            {\"name\": \"linear_layer1\",  \"params\": self.linear_layer1.parameters()},\n",
    "            {\"name\": \"linear_layer2\",  \"params\": self.linear_layer2.parameters()}\n",
    "        ]\n",
    "        if self.semi_sup:\n",
    "            parameters.append(\n",
    "                {\"name\": \"backbone1\", \"params\": self.backbone1.parameters()}\n",
    "            )\n",
    "            parameters.append(\n",
    "                {\"name\": \"backbone2\", \"params\": self.backbone2.parameters()}\n",
    "            )\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    def co_divide_GMM(self, net1: nn.Module, net2: nn.Module, dataloader: data.DataLoader):\n",
    "        \"\"\"implements divide mix GMM algorithm using two networks and a dataset\"\"\"\n",
    "        \n",
    "        # return list(range(int(len(self.datamod_.val_ds)/2.))), list(range(len(self.datamod_.val_ds)-int(len(self.datamod_.val_ds)/2.))),\\\n",
    "        #     list(range(int(len(self.datamod_.val_ds)/2.))), list(range(len(self.datamod_.val_ds)-int(len(self.datamod_.val_ds)/2.))),\\\n",
    "        #     torch.rand((int(len(self.datamod_.val_ds)/2.)), device=self.device), torch.rand((len(self.datamod_.val_ds)-int(len(self.datamod_.val_ds)/2.)), device=self.device)\n",
    "        \n",
    "        \n",
    "        from sklearn.mixture import GaussianMixture\n",
    "        net1.eval()\n",
    "        net2.eval()\n",
    "\n",
    "        losses1 = []\n",
    "        losses2 = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in tqdm(enumerate(dataloader), desc=\"GMM on training sets\", total=len(dataloader)):\n",
    "                (X1, X2) , pos, y, metadata = batch\n",
    "                X1 = X1.to(self.device)\n",
    "                # X2 = X2.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # todo augmented data is passed to the network\n",
    "                (logits1, logits2), _ = self._forward(X1)\n",
    "                \n",
    "                loss1 = F.cross_entropy(logits1, y, reduction='none')\n",
    "                loss2 = F.cross_entropy(logits2, y, reduction='none')\n",
    "                \n",
    "                losses1.append(loss1.detach())\n",
    "                losses2.append(loss2.detach())\n",
    "        \n",
    "        losses1 = torch.cat(losses1).reshape(-1,1).cpu().numpy()\n",
    "        losses2 = torch.cat(losses2).reshape(-1,1).cpu().numpy()\n",
    "        # losses1 = np.concatenate(losses1).reshape(-1,1)\n",
    "        # losses2 = np.concatenate(losses2).reshape(-1,1)\n",
    "        \n",
    "        losses1 = (losses1 - losses1.min())/(losses1.max() - losses1.min())\n",
    "        losses2 = (losses2 - losses2.min())/(losses2.max() - losses2.min())\n",
    "        \n",
    "        self.train_lossHistory1.append(losses1)\n",
    "        self.train_lossHistory2.append(losses2)\n",
    "        \n",
    "        if len(self.train_lossHistory1) > 5:\n",
    "            self.train_lossHistory1 = self.train_lossHistory1[-5:]\n",
    "            self.train_lossHistory2 = self.train_lossHistory2[-5:]\n",
    "            input_loss1 = np.concatenate(self.train_lossHistory1[-5:], axis=0).reshape(-1,1)\n",
    "            input_loss2 = np.concatenate(self.train_lossHistory2[-5:], axis=0).reshape(-1,1)\n",
    "        else:\n",
    "            input_loss1 = np.concatenate(self.train_lossHistory1, axis=0).reshape(-1,1)\n",
    "            input_loss2 = np.concatenate(self.train_lossHistory2, axis=0).reshape(-1,1)\n",
    "        \n",
    "    \n",
    "        GMM1 = GaussianMixture(n_components=2,max_iter=10,tol=1e-2,reg_covar=5e-4)\n",
    "        GMM2 = GaussianMixture(n_components=2,max_iter=10,tol=1e-2,reg_covar=5e-4)\n",
    "    \n",
    "        GMM1.fit(input_loss2)\n",
    "        GMM2.fit(input_loss1)\n",
    "        \n",
    "        prob1 = GMM1.predict_proba(input_loss1) \n",
    "        prob1 = prob1[:,GMM1.means_.argmin()] \n",
    "        prob1 = torch.from_numpy(prob1).to(self.device).view(-1,1)\n",
    "        \n",
    "        prob2 = GMM2.predict_proba(input_loss2) \n",
    "        prob2 = prob2[:,GMM2.means_.argmin()] \n",
    "        prob2 = torch.from_numpy(prob2).to(self.device).view(-1,1)\n",
    "               \n",
    "               \n",
    "        labeled_train_idx1 = np.where(prob1 > self.GMM_prob_threshold)[0]\n",
    "        unlabeled_train_idx1 = np.where(prob1 <= self.GMM_prob_threshold)[0] \n",
    "        \n",
    "        labeled_train_idx2 = np.where(prob2 > self.GMM_prob_threshold)[0]\n",
    "        unlabeled_train_idx2 = np.where(prob2 <= self.GMM_prob_threshold)[0] \n",
    "        return labeled_train_idx1, unlabeled_train_idx1, labeled_train_idx2, unlabeled_train_idx2, prob1, prob2\n",
    "    \n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        \"\"\"\n",
    "        Changing model to eval() mode has to happen at the start of every epoch,\n",
    "        and should only happen if we are not in semi-supervised mode\n",
    "        \"\"\"\n",
    "        if not self.semi_sup:\n",
    "            self.backbone1.eval()\n",
    "            self.backbone2.eval()\n",
    "        \n",
    "        # every n epochs, co-divide the dataset to labeled and unlabeled sets\n",
    "        # if self.trainer.current_epoch%self.GMM_cycle == 0:\n",
    "        if self.currentepoch%self.GMM_cycle == 0:\n",
    "            # self.trainer.datamodule.labeled_train_idx, self.trainer.datamodule.ulabeled_train_idx,\\\n",
    "            self.datamod_.labeled_train_idx1, self.datamod_.ulabeled_train_idx1,\\\n",
    "            self.datamod_.labeled_train_idx2, self.datamod_.ulabeled_train_idx2,\\\n",
    "            self.GMM_labeled_probs1, self.GMM_labeled_probs2 = self.co_divide_GMM(\n",
    "                    nn.ModuleList([self.backbone1, self.linear_layer1]),\n",
    "                    nn.ModuleList([self.backbone2, self.linear_layer2]),\n",
    "                    # self.trainer.datamodule.train_GMM_loader\n",
    "                    self.datamod_.train_GMM_loader\n",
    "                )\n",
    "\n",
    "    def shared_step(self, batch):\n",
    "        x, pos, y, metadata = batch\n",
    "\n",
    "        with nullcontext() if self.semi_sup else torch.no_grad():\n",
    "            feats1, feats2 = self.get_feats(x)\n",
    "\n",
    "        feats1 = feats1.view(feats1.size(0), -1)\n",
    "        feats2 = feats2.view(feats2.size(0), -1)\n",
    "        \n",
    "        logits1 = self.linear_layer1(feats1)\n",
    "        logits2 = self.linear_layer1(feats2)\n",
    "        \n",
    "        loss1 = F.cross_entropy(logits1, y)\n",
    "        loss2 = F.cross_entropy(logits2, y)\n",
    "\n",
    "        return (loss1, loss2), (logits1,logits2), y, metadata\n",
    "\n",
    "    def get_feats(self, x, network_number=None):\n",
    "        if network_number == 1:\n",
    "            return self.backbone1.get_features(x)\n",
    "        elif network_number == 2:\n",
    "            return self.backbone2.get_features(x)\n",
    "        \n",
    "        return self.backbone1.get_features(x), self.backbone2.get_features(x)\n",
    "\n",
    "    def _forward(self, x, network_number=None):\n",
    "        with nullcontext() if self.semi_sup else torch.no_grad():\n",
    "            if network_number == 1:\n",
    "                feats1 = self.get_feats(x, network_number=1)\n",
    "                feats1 = feats1.view(feats1.size(0), -1)\n",
    "                \n",
    "                logits1 = self.linear_layer1(feats1)\n",
    "                return logits1, feats1\n",
    "            elif network_number == 2:\n",
    "                feats2 = self.get_feats(x, network_number=2)\n",
    "                feats2 = feats2.view(feats2.size(0), -1)\n",
    "                \n",
    "                logits2 = self.linear_layer2(feats2)\n",
    "                return logits2, feats2\n",
    "            else:\n",
    "                feats1, feats2 = self.get_feats(x)\n",
    "                feats1 = feats1.view(feats1.size(0), -1)\n",
    "                feats2 = feats2.view(feats2.size(0), -1)\n",
    "\n",
    "                logits1 = self.linear_layer1(feats1)\n",
    "                logits2 = self.linear_layer2(feats2)\n",
    "                return (logits1,logits2), (feats1,feats2)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx, net_no=1):\n",
    "        if self.semi_sup:\n",
    "            self.backbone1.train()\n",
    "            self.backbone2.train()\n",
    "        self.linear_layer1.train()\n",
    "        self.linear_layer2.train()\n",
    "        \n",
    "        labeled_batch = batch[\"labeled\"]\n",
    "        unlabeled_batch = batch[\"unlabeled\"]\n",
    "        \n",
    "        (X1, X2), pos, labeled_labels, metadata = labeled_batch\n",
    "        (X1_unlabeled, X2_unlabeled), pos, unlabeled_labels, metadata = unlabeled_batch\n",
    "        batch_size = X1.size(0)\n",
    "        \n",
    "        # todo remove after adding the code to the pipeline\n",
    "        X1, X2, X1_unlabeled, X2_unlabeled = X1.to(self.device), X2.to(self.device), X1_unlabeled.to(self.device), X2_unlabeled.to(self.device)\n",
    "        labeled_labels = labeled_labels.to(self.device)\n",
    "        labeled_labels = F.one_hot(labeled_labels, num_classes=self.num_classes).float()\n",
    "        \n",
    "        start_indx = batch_idx*batch_size\n",
    "        end_indx = (batch_idx+1)*batch_size\n",
    "        if net_no == 1:\n",
    "            w_x = self.GMM_labeled_probs1[start_indx:end_indx].view(-1,1)\n",
    "        elif net_no == 2:\n",
    "            w_x = self.GMM_labeled_probs2[start_indx:end_indx].view(-1,1)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # get unlablled data pseudo labels\n",
    "            (logits1_unlabeled_model1, logits1_unlabeled_model2), _ = self._forward(X1_unlabeled)\n",
    "            (logits2_unlabeled_model1, logits2_unlabeled_model2), _ = self._forward(X2_unlabeled)\n",
    "        \n",
    "            unlabeled_avg_probs = (torch.softmax(logits1_unlabeled_model1, dim=1) + \n",
    "                                    torch.softmax(logits1_unlabeled_model2, dim=1) +\n",
    "                                    torch.softmax(logits2_unlabeled_model1, dim=1) + \n",
    "                                    torch.softmax(logits2_unlabeled_model2, dim=1)) / 4\n",
    "            unlabeled_pseudo_targets = unlabeled_avg_probs**(1/self.temperature)\n",
    "            unlabeled_pseudo_targets = unlabeled_pseudo_targets / unlabeled_pseudo_targets.sum(dim=1, keepdim=True) # normalize\n",
    "            unlabeled_pseudo_targets = unlabeled_pseudo_targets.detach()    \n",
    "        \n",
    "            # get lablled data pseudo labels\n",
    "            logits1_labeled_model1, _ = self._forward(X1, network_number=net_no)\n",
    "            logits2_labeled_model1, _ = self._forward(X2, network_number=net_no)\n",
    "        \n",
    "        \n",
    "\n",
    "        labeled_avg_probs = (torch.softmax(logits1_labeled_model1, dim=1) + \n",
    "                                 torch.softmax(logits2_labeled_model1, dim=1)) / 2\n",
    "        # finding labels for labeled data using GMM1\n",
    "        labeled_avg_probs1 = w_x*labeled_labels + (1-w_x)*labeled_avg_probs              \n",
    "        labeled_pseudo_targets1 = labeled_avg_probs1**(1/self.temperature) # temparature sharpening \n",
    "        labeled_pseudo_targets1 = labeled_pseudo_targets1 / labeled_pseudo_targets1.sum(dim=1, keepdim=True) # normalize           \n",
    "        labeled_pseudo_targets1 = labeled_pseudo_targets1.detach()       \n",
    "        \n",
    "        \n",
    "        # mixmatch\n",
    "        _lambda = np.random.beta(self.beta_alpha, self.beta_alpha)        \n",
    "        _lambda = max(_lambda, 1-_lambda)\n",
    "                \n",
    "        all_inputs = torch.cat([X1, X2, X1_unlabeled, X2_unlabeled], dim=0)\n",
    "        all_targets1 = torch.cat([labeled_pseudo_targets1, labeled_pseudo_targets1, unlabeled_pseudo_targets, unlabeled_pseudo_targets], dim=0)\n",
    "\n",
    "        idx = torch.randperm(all_inputs.size(0))\n",
    "\n",
    "        input_a, input_b = all_inputs, all_inputs[idx]\n",
    "        target_a1, target_b1 = all_targets1, all_targets1[idx]\n",
    "        \n",
    "        mixed_input = _lambda * input_a + (1 - _lambda) * input_b        \n",
    "        mixed_target1 = _lambda * target_a1 + (1 - _lambda) * target_b1\n",
    "                \n",
    "        \n",
    "        logits_mixed_model1, _ = self._forward(mixed_input, network_number=net_no)\n",
    "        logits_labeled1 = logits_mixed_model1[:batch_size*2]\n",
    "        logits_unlabeled1 = logits_mixed_model1[batch_size*2:]        \n",
    "        \n",
    "        \n",
    "        loss_labeled1, loss_unlabeled1, lambda_unlabeled1 = self.semi_loss(logits_labeled1, mixed_target1[:batch_size*2], \n",
    "                                                                           logits_unlabeled1, mixed_target1[batch_size*2:], \n",
    "                                                                           #  self.trainer.current_epoch+batch_idx/self.num_training_steps\n",
    "                                                                           self.currentepoch+batch_idx/self.num_training_steps\n",
    "                                                                           )\n",
    "        \n",
    "        # regularization\n",
    "        prior = torch.ones(self.num_classes)/self.num_classes\n",
    "        prior = prior.to(self.device)      \n",
    "          \n",
    "        pred_mean1 = torch.softmax(logits_mixed_model1, dim=1).mean(0)\n",
    "        penalty1 = torch.sum(prior*torch.log(prior/pred_mean1))\n",
    "\n",
    "        # loss\n",
    "        loss = loss_labeled1 + lambda_unlabeled1 * loss_unlabeled1  + penalty1\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx: int):\n",
    "        loss, logits, y, *metadata = self.shared_step(batch)\n",
    "\n",
    "        self.logging_combined_centers_loss(dataloader_idx, loss)\n",
    "\n",
    "        return loss, logits, y, *metadata\n",
    "\n",
    "    def validation_epoch_end(self, outs):        \n",
    "        kwargs = {\n",
    "            \"on_step\": False,\n",
    "            \"on_epoch\": True,\n",
    "            \"sync_dist\": True,\n",
    "            \"add_dataloader_idx\": False,\n",
    "        }\n",
    "        self.log(\n",
    "            \"val/finetune_loss\",\n",
    "            torch.mean(torch.tensor(self.val_macroLoss_all_centers)),\n",
    "            prog_bar=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test/finetune_loss\",\n",
    "            torch.mean(torch.tensor(self.test_macroLoss_all_centers)),\n",
    "            prog_bar=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, logits, y, *metadata = self.shared_step(batch)\n",
    "        return loss, logits, y, *metadata\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.train_acc.reset()\n",
    "\n",
    "        self.val_macroLoss_all_centers = []\n",
    "        self.test_macroLoss_all_centers = []\n",
    "        \n",
    "    def logging_combined_centers_loss(self, dataloader_idx, loss):\n",
    "        \"\"\"macro loss for centers\"\"\"\n",
    "        self.inferred_no_centers = (\n",
    "            dataloader_idx + 1\n",
    "            if dataloader_idx + 1 > self.inferred_no_centers\n",
    "            else self.inferred_no_centers\n",
    "        )\n",
    "\n",
    "        if dataloader_idx < self.inferred_no_centers / 2.0:\n",
    "            self.val_macroLoss_all_centers.append(loss)\n",
    "        else:\n",
    "            self.test_macroLoss_all_centers.append(loss)\n",
    "\n",
    "    def semi_loss(self, outputs_labeled, targets_labeled, outputs_unlabeled, targets_unlabeled, epoch, warm_up=0):\n",
    "        probs_u = torch.softmax(outputs_unlabeled, dim=1)\n",
    "\n",
    "        loss_labeled = -torch.mean(torch.sum(F.log_softmax(outputs_labeled, dim=1) * targets_labeled, dim=1))\n",
    "        loss_unlabeled = torch.mean((probs_u - targets_unlabeled)**2)\n",
    "\n",
    "        def linear_rampup(current, warm_up=0, rampup_length=16):\n",
    "            current = np.clip((current-warm_up) / rampup_length, 0.0, 1.0)\n",
    "            return 25.0*float(current)\n",
    "\n",
    "        return loss_labeled, loss_unlabeled, linear_rampup(epoch, warm_up)\n",
    "\n",
    "    @property\n",
    "    def num_training_steps(self) -> int:\n",
    "        return len(self.datamod_.train_ds) // self.batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividemix module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdigilany/anaconda3/envs/exact_env/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'backbone' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['backbone'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "model_conf1 = OmegaConf.create({\"_target_\": \"src.modeling.registry.create_model\",\n",
    "                                \"_recursive_\": False,\n",
    "                                \"model_name\": \"finetune_linear_all_centers\",\n",
    "                                \"seed\": 0,\n",
    "                                \"split_seed\": 0,\n",
    "                                })\n",
    "model_conf2 = OmegaConf.create({\"_target_\": \"src.modeling.registry.create_model\",\n",
    "                                \"_recursive_\": False,\n",
    "                                \"model_name\": \"finetune_linear_all_centers\",\n",
    "                                \"seed\": 0,\n",
    "                                \"split_seed\": 1,\n",
    "                                })\n",
    "\n",
    "dividemix_model = DivideMix(model_conf1, model_conf2).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GMM on training sets: 100%|██████████| 3328/3328 [03:15<00:00, 16.99it/s]\n",
      "batch: 100%|██████████| 3295/3295 [08:50<00:00,  6.21it/s]  \n",
      "batch: 100%|██████████| 3235/3235 [08:01<00:00,  6.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# loading adam optimizer\n",
    "from torch.optim import Adam\n",
    "optim = Adam(dividemix_model.learnable_parameters, lr=0.0001, weight_decay=1e-6)\n",
    "\n",
    "# training dividemix model for 100 epochs\n",
    "epochs = 1\n",
    "for i in range(epochs):\n",
    "    train_loss = []\n",
    "    dividemix_model.currentepoch = i\n",
    "    dividemix_model.datamod_ = lightning_data_module\n",
    "    \n",
    "    # train first model\n",
    "    train_loader = lightning_data_module.train_dataloader(model=1)\n",
    "    \n",
    "    labeled_loader, unlabeled_loader =\\\n",
    "    train_loader[\"labeled\"], train_loader[\"unlabeled\"]\n",
    "    dividemix_model.on_train_epoch_start()\n",
    "    \n",
    "    unlabeled_loader_iter = iter(unlabeled_loader)\n",
    "    for batch_idx, batch_labeled in tqdm(enumerate(labeled_loader), desc=\"batch\", total=len(labeled_loader)):\n",
    "        try:\n",
    "            batch_unlabeled = next(unlabeled_loader_iter)\n",
    "        except:\n",
    "            unlabeled_loader_iter = iter(unlabeled_loader)\n",
    "            batch_unlabeled = next(unlabeled_loader_iter)\n",
    "        loss = dividemix_model.training_step({\"labeled\":batch_labeled, \n",
    "                                              \"unlabeled\": batch_unlabeled,\n",
    "                                              }, batch_idx)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    \n",
    "    # train 2nd model\n",
    "    train_loader = lightning_data_module.train_dataloader(model=2)\n",
    "    \n",
    "    labeled_loader, unlabeled_loader =\\\n",
    "    train_loader[\"labeled\"], train_loader[\"unlabeled\"]   \n",
    "    \n",
    "    unlabeled_loader_iter = iter(unlabeled_loader)\n",
    "    for batch_idx, batch_labeled in tqdm(enumerate(labeled_loader), desc=\"batch\", total=len(labeled_loader)):\n",
    "        try:\n",
    "            batch_unlabeled = next(unlabeled_loader_iter)\n",
    "        except:\n",
    "            unlabeled_loader_iter = iter(unlabeled_loader)\n",
    "            batch_unlabeled = next(unlabeled_loader_iter)\n",
    "        loss = dividemix_model.training_step({\"labeled\":batch_labeled, \n",
    "                                              \"unlabeled\": batch_unlabeled,\n",
    "                                              }, batch_idx)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        train_loss.append(loss.detach())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5549, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3328"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lightning_data_module.train_GMM_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3295"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = lightning_data_module.train_dataloader()\n",
    "labeled_loader, unlabeled_loader = train_loader[\"labeled\"], train_loader[\"unlabeled\"]\n",
    "len(labeled_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n",
      "3328\n"
     ]
    }
   ],
   "source": [
    "print(len(lightning_data_module.unlabeled_train_idx))\n",
    "print(len(lightning_data_module.train_GMM_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exact_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "410e89f2f44b0beb9bc17729029af0a1782becd3423b288ae69b52d70a74bed6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
