{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This cell contains the code we've implemented. You should be able to call each function directly, or alternatively, see our example calls below\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from sklearn.datasets import make_sparse_coded_signal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "# from test_omp import omp_naive\n",
    "# from test import *  # FIXME: better name\n",
    "from line_profiler import line_profiler\n",
    "\n",
    "n_components, n_features = 100, 100\n",
    "n_nonzero_coefs = 17\n",
    "n_samples = 50\n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    # https://stackoverflow.com/questions/7370801/how-to-measure-elapsed-time-in-python\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "\n",
    "\n",
    "def run_omp(X, y, n_nonzero_coefs, precompute=True, tol=0.0, normalize=False, fit_intercept=False, alg='naive'):\n",
    "    if not isinstance(X, torch.Tensor):\n",
    "        X = torch.as_tensor(X)\n",
    "        y = torch.as_tensor(y)\n",
    "\n",
    "    # We can either return sets, (sets, solutions), or xests\n",
    "    # These are all equivalent, but are simply more and more dense representations.\n",
    "    # Given sets and X and y one can (re-)construct xests. The second is just a sparse vector repr.\n",
    "\n",
    "    # https://github.com/scikit-learn/scikit-learn/blob/15a949460dbf19e5e196b8ef48f9712b72a3b3c3/sklearn/linear_model/_omp.py#L690\n",
    "    if fit_intercept or normalize:\n",
    "        X = X.clone()\n",
    "        assert not isinstance(precompute, torch.Tensor), \"If user pre-computes XTX they can also pre-normalize X\" \\\n",
    "                                                         \" as well, so normalize and fit_intercept must be set false.\"\n",
    "\n",
    "    if fit_intercept:\n",
    "        X = X - X.mean(0)\n",
    "        y = y - y.mean(1)[:, None]\n",
    "\n",
    "    # To keep a good condition number on X, especially with Cholesky compared to LU factorization,\n",
    "    # we should probably always normalize it (OMP is invariant anyways)\n",
    "    if normalize is True:  # User can also just optionally supply pre-computed norms.\n",
    "        normalize = (X * X).sum(0).sqrt()\n",
    "        X /= normalize[None, :]\n",
    "\n",
    "    if precompute is True or alg == 'v0':\n",
    "        precompute = X.T @ X\n",
    "\n",
    "    # If n_nonzero_coefs is equal to M, one should just return lstsq\n",
    "    if alg == 'naive':\n",
    "        sets, solutions, lengths = omp_naive(X, y, n_nonzero_coefs=n_nonzero_coefs, XTX=precompute, tol=tol)\n",
    "    elif alg == 'v0':\n",
    "        sets, solutions, lengths = omp_v0(X, y, n_nonzero_coefs=n_nonzero_coefs, XTX=precompute, tol=tol)\n",
    "\n",
    "\n",
    "    solutions = solutions.squeeze(-1)\n",
    "    if normalize is not False:\n",
    "        solutions /= normalize[sets]\n",
    "\n",
    "    xests = y.new_zeros(y.shape[0], X.shape[1])\n",
    "    if lengths is None:\n",
    "        xests[torch.arange(y.shape[0], dtype=sets.dtype, device=sets.device)[:, None], sets] = solutions\n",
    "    else:\n",
    "        for i in range(y.shape[0]):\n",
    "            # xests[i].scatter_(-1, sets[i, :lengths[i]], solutions[i, :lengths[i]])\n",
    "            xests[i, sets[i, :lengths[i]]] = solutions[i, :lengths[i]]\n",
    "\n",
    "    return xests\n",
    "\n",
    "def batch_mm(matrix, matrix_batch, return_contiguous=True):\n",
    "    \"\"\"\n",
    "    :param matrix: Sparse or dense matrix, size (m, n).\n",
    "    :param matrix_batch: Batched dense matrices, size (b, n, k).\n",
    "    :return: The batched matrix-matrix product, size (m, n) x (b, n, k) = (b, m, k).\n",
    "    \"\"\"\n",
    "    # One dgemm is faster than many dgemv.\n",
    "    # From https://github.com/pytorch/pytorch/issues/14489#issuecomment-607730242\n",
    "    batch_size = matrix_batch.shape[0]\n",
    "    # Stack the vector batch into columns. (b, n, k) -> (n, b, k) -> (n, b*k)\n",
    "    vectors = matrix_batch.transpose([1, 0, 2]).reshape(matrix.shape[1], -1)\n",
    "\n",
    "    # A matrix-matrix product is a batched matrix-vector product of the columns.\n",
    "    # And then reverse the reshaping. (m, n) x (n, b*k) = (m, b*k) -> (m, b, k) -> (b, m, k)\n",
    "    if return_contiguous:\n",
    "        result = np.empty_like(matrix_batch, shape=(batch_size, matrix.shape[0], matrix_batch.shape[2]))\n",
    "        np.matmul(matrix, vectors, out=result.transpose([1, 0, 2]).reshape(matrix.shape[0], -1))\n",
    "    else:\n",
    "        result = (matrix @ vectors).reshape(matrix.shape[0], batch_size, -1).transpose([1, 0, 2])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def innerp(x, y=None, out=None):\n",
    "    if y is None:\n",
    "        y = x\n",
    "    if out is not None:\n",
    "        out = out[:, None, None]  # Add space for two singleton dimensions.\n",
    "    return torch.matmul(x[..., None, :], y[..., :, None], out=out)[..., 0, 0]\n",
    "\n",
    "def cholesky_solve(ATA, ATy):\n",
    "    if ATA.dtype == torch.half or ATy.dtype == torch.half:\n",
    "        return ATy.to(torch.float).cholesky_solve(torch.cholesky(ATA.to(torch.float))).to(ATy.dtype)\n",
    "    return ATy.cholesky_solve(torch.cholesky(ATA)).to(ATy.dtype)\n",
    "\n",
    "\n",
    "def omp_naive(X, y, n_nonzero_coefs, tol=None, XTX=None):\n",
    "    on_cpu = not (y.is_cuda or y.dtype == torch.half)\n",
    "    # torch.cuda.synchronize()\n",
    "    # Given X as an MxN array and y as an BxN array, do omp to approximately solve Xb=y\n",
    "\n",
    "    # Base variables\n",
    "    XT = X.contiguous().t()  # Store XT in fortran-order.\n",
    "    y = y.contiguous()\n",
    "    r = y.clone()\n",
    "\n",
    "    sets = y.new_zeros((n_nonzero_coefs, y.shape[0]), dtype=torch.long).t()\n",
    "    if tol:\n",
    "        result_sets = sets.new_zeros(y.shape[0], n_nonzero_coefs)\n",
    "        result_lengths = sets.new_zeros(y.shape[0])\n",
    "        result_solutions = y.new_zeros((y.shape[0], n_nonzero_coefs, 1))\n",
    "        original_indices = torch.arange(y.shape[0], dtype=sets.dtype, device=sets.device)\n",
    "\n",
    "    # Trade b*k^2+bk+bkM = O(bkM) memory for much less compute time. (This has to be done anyways since we are batching,\n",
    "    # otherwise one could just permute columns of X in-place as in https://github.com/scikit-learn/scikit-learn/blob/15a949460dbf19e5e196b8ef48f9712b72a3b3c3/sklearn/linear_model/_omp.py#L28 )\n",
    "    ATs = y.new_zeros(r.shape[0], n_nonzero_coefs, X.shape[0])\n",
    "    ATys = y.new_zeros(r.shape[0], n_nonzero_coefs, 1)\n",
    "    ATAs = torch.eye(n_nonzero_coefs, dtype=y.dtype, device=y.device)[None].repeat(r.shape[0], 1, 1)\n",
    "    if on_cpu:\n",
    "        # For CPU it is faster to use a packed representation of the lower triangle in ATA.\n",
    "        tri_idx = torch.tril_indices(n_nonzero_coefs, n_nonzero_coefs, device=sets.device, dtype=sets.dtype)\n",
    "        ATAs = ATAs[:, tri_idx[0], tri_idx[1]]\n",
    "\n",
    "    solutions = y.new_zeros((r.shape[0], 0))\n",
    "\n",
    "    for k in range(n_nonzero_coefs+bool(tol)):\n",
    "        # STOPPING CRITERIA\n",
    "        if tol:\n",
    "            problems_done = innerp(r) <= tol\n",
    "            if k == n_nonzero_coefs:\n",
    "                problems_done[:] = True\n",
    "\n",
    "            if problems_done.any():\n",
    "                remaining = ~problems_done\n",
    "\n",
    "                orig_idxs = original_indices[problems_done]\n",
    "                result_sets[orig_idxs, :k] = sets[problems_done, :k]\n",
    "                result_solutions[orig_idxs, :k] = solutions[problems_done]\n",
    "                result_lengths[orig_idxs] = k\n",
    "                original_indices = original_indices[remaining]\n",
    "\n",
    "                # original_indices = original_indices[remaining]\n",
    "                ATs = ATs[remaining]\n",
    "                ATys = ATys[remaining]\n",
    "                ATAs = ATAs[remaining]\n",
    "                sets = sets[remaining]\n",
    "                y = y[remaining]\n",
    "                r = r[remaining]\n",
    "                if problems_done.all():\n",
    "                    return result_sets, result_solutions, result_lengths\n",
    "        # GET PROJECTIONS AND INDICES TO ADD\n",
    "        if on_cpu:\n",
    "            projections = batch_mm(XT.numpy(), r[:, :, None].numpy())\n",
    "            argmax_blast(projections.squeeze(-1), sets[:, k].numpy())\n",
    "        else:\n",
    "            projections = XT @ r[:, :, None]\n",
    "            sets[:, k] = projections.abs().sum(-1).argmax(-1)  # Sum is just a squeeze, but would be relevant in SOMP.\n",
    "\n",
    "        # UPDATE AT\n",
    "        AT = ATs[:, :k + 1, :]\n",
    "        updateA = XT[sets[:, k], :]\n",
    "        AT[:, k, :] = updateA\n",
    "\n",
    "        # UPDATE ATy based on AT\n",
    "        ATy = ATys[:, :k + 1]\n",
    "        ATy[:, k, 0] = innerp(updateA, y)\n",
    "\n",
    "        # UPDATE ATA based on AT or precomputed XTX.\n",
    "        if on_cpu:\n",
    "            packed_idx = k * (k - 1) // 2\n",
    "            if XTX is not None:  # Update based on precomputed XTX.\n",
    "                ATAs.t()[k + packed_idx:packed_idx + 2 * k + 1, :].t().numpy()[:] = XTX[sets[:, k, None], sets[:, :k + 1]]\n",
    "            else:\n",
    "                np.matmul(AT[:, :k + 1, :].numpy(), updateA[:, :, None].numpy(),\n",
    "                          out=ATAs.t()[k + packed_idx:packed_idx + 2 * k + 1, :].t()[:, :, None].numpy())\n",
    "        else:\n",
    "            ATA = ATAs[:, :k + 1, :k + 1]\n",
    "            if XTX is not None:\n",
    "                ATA[:, k, :k + 1] = XTX[sets[:, k, None], sets[:, :k + 1]]\n",
    "            else:\n",
    "                # Update ATAs by adding the new column of inner products.\n",
    "                torch.bmm(AT[:, :k + 1, :], updateA[:, :, None], out=ATA[:, k, :k + 1, None])\n",
    "\n",
    "        # SOLVE ATAx = ATy.\n",
    "        if on_cpu:\n",
    "            solutions = ATy.permute(0, 2, 1).clone().permute(0, 2, 1)  # Get a copy.\n",
    "            ppsv(ATAs.t()[:packed_idx + 2 * k + 1, :].t().contiguous().numpy(), solutions.numpy())\n",
    "        else:\n",
    "            ATA[:, :k, k] = ATA[:, k, :k]  # Copy lower triangle to upper triangle.\n",
    "            # solutions = cholesky_solve(ATA, ATy)\n",
    "            solutions = torch.linalg.solve(ATA, ATy) # (batch_sz, n_nonzero_coefs, 1)\n",
    "\n",
    "\n",
    "        # FINALLY, GET NEW RESIDUAL r=y-Ax\n",
    "        if on_cpu:\n",
    "            np.subtract(y.numpy(), (AT.permute(0, 2, 1).numpy() @ solutions.numpy()).squeeze(-1), out=r.numpy())\n",
    "        else:\n",
    "            r[:, :, None] = torch.baddbmm(y[:, :, None], AT.permute(0, 2, 1), solutions, beta=-1)\n",
    "        a = 0\n",
    "        \n",
    "    return sets, solutions, None\n",
    "\n",
    "def omp_v0(X, y, XTX, n_nonzero_coefs=None, tol=None, inverse_cholesky=True):\n",
    "    B = y.shape[0]\n",
    "    normr2 = innerp(y)  # Norm squared of residual.\n",
    "    projections = (X.transpose(1, 0) @ y[:, :, None]).squeeze(-1)\n",
    "    sets = y.new_zeros(n_nonzero_coefs, B, dtype=torch.int64)\n",
    "\n",
    "    if inverse_cholesky:\n",
    "        # Doing the inverse-cholesky iteratively uses more memory,\n",
    "        # but takes less time than waiting till solving the problem in the end it seems.\n",
    "        # (Since F is triangular it could be __even faster__ to multiply, prob. not on GPU tho.)\n",
    "        F = torch.eye(n_nonzero_coefs, dtype=y.dtype, device=y.device).repeat(B, 1, 1)\n",
    "        a_F = y.new_zeros(n_nonzero_coefs, B, 1)\n",
    "\n",
    "    D_mybest = y.new_empty(B, n_nonzero_coefs, XTX.shape[0])\n",
    "    temp_F_k_k = y.new_ones((B, 1))\n",
    "\n",
    "    if tol:\n",
    "        result_lengths = sets.new_zeros(y.shape[0])\n",
    "        result_solutions = y.new_zeros((y.shape[0], n_nonzero_coefs, 1))\n",
    "        finished_problems = sets.new_zeros(y.shape[0], dtype=torch.bool)\n",
    "\n",
    "    for k in range(n_nonzero_coefs+bool(tol)):\n",
    "        # STOPPING CRITERIA\n",
    "        if tol:\n",
    "            problems_done = normr2 <= tol\n",
    "            if k == n_nonzero_coefs:\n",
    "                problems_done[:] = True\n",
    "\n",
    "            if problems_done.any():\n",
    "                new_problems_done = problems_done & ~finished_problems\n",
    "                finished_problems.logical_or_(problems_done)\n",
    "                result_lengths[new_problems_done] = k\n",
    "                if inverse_cholesky:\n",
    "                    result_solutions[new_problems_done, :k] = F[new_problems_done, :k, :k].permute(0, 2, 1) @ a_F[:k, new_problems_done].permute(1, 0, 2)\n",
    "                else:\n",
    "                    assert False, \"inverse_cholesky=False with tol != None is not handled yet\"\n",
    "                if problems_done.all():\n",
    "                    return sets.t(), result_solutions, result_lengths\n",
    "\n",
    "        sets[k] = projections.abs().argmax(1)\n",
    "        # D_mybest[:, k, :] = XTX[gamma[k], :]  # Same line as below, but significantly slower. (prob. due to the intermediate array creation)\n",
    "        torch.gather(XTX, 0, sets[k, :, None].expand(-1, XTX.shape[1]), out=D_mybest[:, k, :])\n",
    "        if k:\n",
    "            D_mybest_maxindices = D_mybest.permute(0, 2, 1)[torch.arange(D_mybest.shape[0], dtype=sets.dtype, device=sets.device), sets[k], :k]\n",
    "            torch.rsqrt(1 - innerp(D_mybest_maxindices),\n",
    "                        out=temp_F_k_k[:, 0])  # torch.exp(-1/2 * torch.log1p(-inp), temp_F_k_k[:, 0])\n",
    "            D_mybest_maxindices *= -temp_F_k_k  # minimal operations, exploit linearity\n",
    "            D_mybest[:, k, :] *= temp_F_k_k\n",
    "            D_mybest[:, k, :, None].baddbmm_(D_mybest[:, :k, :].permute(0, 2, 1), D_mybest_maxindices[:, :, None])\n",
    "\n",
    "\n",
    "        temp_a_F = temp_F_k_k * torch.gather(projections, 1, sets[k, :, None])\n",
    "        normr2 -= (temp_a_F * temp_a_F).squeeze(-1)\n",
    "        projections -= temp_a_F * D_mybest[:, k, :]\n",
    "        if inverse_cholesky:\n",
    "            a_F[k] = temp_a_F\n",
    "            if k:  # Could maybe get a speedup from triangular mat mul kernel.\n",
    "                torch.bmm(D_mybest_maxindices[:, None, :], F[:, :k, :], out=F[:, k, None, :])\n",
    "                F[:, k, k] = temp_F_k_k[..., 0]\n",
    "    else: # FIXME: else branch will not execute if n_nonzero_coefs=0, so solutions is undefined.\n",
    "        # Normal exit, used when tolerance=None.\n",
    "        if inverse_cholesky:\n",
    "            solutions = F.permute(0, 2, 1) @ a_F.squeeze(-1).transpose(1, 0)[:, :, None]\n",
    "        else:\n",
    "            # Solving the problem in the end without using inverse Cholesky.\n",
    "            AT = X.T[sets.T]\n",
    "            solutions = cholesky_solve(AT @ AT.permute(0, 2, 1), AT @ y.T[:, :, None])\n",
    "\n",
    "    return sets.t(), solutions, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def omp(dict, y, n_nonzero_coefs=15):\n",
    "    '''Orthogonal Matching pursuit algorithm\n",
    "    '''\n",
    "\n",
    "    chunk_length, n_atoms = dict.shape\n",
    "    batch_sz, chunk_length = y.shape\n",
    "    DTD = dict.T @ dict\n",
    "    \n",
    "    \n",
    "    residuals = y.clone()\n",
    "    max_score_indices = y.new_zeros((batch_sz, n_nonzero_coefs), dtype=torch.long) # (batch_sz, n_nonzero_coefs)\n",
    "    \n",
    "    # sparse weight matrix\n",
    "    # W = torch.zeros(batch_sz, chunk_length, n_atoms).to(device=dict.device, dtype=dict.dtype) # (batch_sz, chunk_length, atom_dim)\n",
    "    \n",
    "    sparse_W = torch.zeros(batch_sz, chunk_length, n_nonzero_coefs).to(device=dict.device, dtype=dict.dtype) # (batch_sz, chunk_length, n_nonzero_coefs)\n",
    "    sparse_WTy = torch.zeros(batch_sz, n_nonzero_coefs, 1).to(device=dict.device, dtype=dict.dtype) # (batch_sz, n_nonzero_coefs, 1)\n",
    "    \n",
    "    # sparse_WTW = torch.eye(n_nonzero_coefs, dtype=dict.dtype, device=dict.device)[None].repeat(batch_sz, 1, 1)\n",
    "    sparse_WTW = torch.zeros(batch_sz, n_nonzero_coefs, n_nonzero_coefs).to(device=dict.device, dtype=dict.dtype) # (batch_sz, n_nonzero_coefs, n_nonzero_coefs)\n",
    "    \n",
    "    tolerance = True\n",
    "    # Control stop interation with norm thresh or sparsity\n",
    "    for i in range(n_nonzero_coefs): \n",
    "        # Compute the score of each atoms\n",
    "        projections = dict.T @ residuals[:, :, None] # (batch_sz, input_dim, 1)\n",
    "        max_score_indices[:, i] = projections.abs().sum(-1).argmax(-1) # Sum is just a squeeze, but would be relevant in SOMP.\n",
    "        \n",
    "        # update sparse_W\n",
    "        _sparse_W = sparse_W[:, :, :i + 1]\n",
    "        curr_atoms = dict[:, max_score_indices[:, i]] # select the atom with the max score (chunk_length, batch_sz)\n",
    "        _sparse_W[:, :, i] = curr_atoms.T # an atom is added to sparse W per each datum in the batch at each iteration\n",
    "\n",
    "        # update sparse_WTy based on the current atom\n",
    "        _sparse_WTy = sparse_WTy[:, :i + 1]\n",
    "        _sparse_WTy[:, i, 0] = torch.bmm(_sparse_W[:, :, i:i+1].permute(0, 2, 1), y[:, :, None]).squeeze() # (batch_sz, 1, chunk_length) * (batch_sz, chunk_length, 1) -> (batch_sz, 1, 1) \n",
    "\n",
    "        \n",
    "        # update sparse_WTW based on sparse_WT or precomputed XTX.\n",
    "        _sparse_WTW = sparse_WTW[:, :i + 1, :i + 1]\n",
    "        \n",
    "        # Update sparse_WTW by adding the new column of inner products.\n",
    "        _sparse_WTW[:, :, :] = torch.bmm(_sparse_W[:, :, :i+1].permute(0, 2, 1), _sparse_W[:, :, :i+1])\n",
    "        \n",
    "        # _sparse_WTW[:, i, :i + 1] = DTD[max_score_indices[:, i, None], max_score_indices[:, :i + 1]]\n",
    "        # _sparse_WTW[:, i, :i + 1, None] = torch.bmm(_sparse_W[:, :, :i+1].permute(0, 2, 1), curr_atoms.T[:, :, None])\n",
    "        # _sparse_WTW[:, :i, i] = _sparse_WTW[:, i, :i] # Copy lower triangle to upper triangle.\n",
    "        \n",
    "        # solve sparse_W @ solutions = y\n",
    "        # solutions = cholesky_solve(_sparse_WTW, _sparse_WTy)\n",
    "        # linear solve\n",
    "        # _sparse_WTW.diagonal(dim1=-2, dim2=-1).add_(F.softplus(torch.tensor(-15.0))) # TODO: add multipath OMP\n",
    "        solutions = torch.linalg.solve(_sparse_WTW, _sparse_WTy) # (batch_sz, n_nonzero_coefs, 1)\n",
    "\n",
    "        # finally get residuals r=y-Wx\n",
    "        # residuals[:, :, None]  = torch.baddbmm(y[:, :, None], _sparse_W, solutions, beta=-1)\n",
    "        residuals[:, :, None] = y[:, :, None] - _sparse_W @ solutions\n",
    "        a = 0\n",
    "    \n",
    "    solutions = solutions.squeeze(-1)\n",
    "    W = y.new_zeros(batch_sz, n_atoms)\n",
    "    W[torch.arange(batch_sz, dtype=max_score_indices.dtype, device=max_score_indices.device)[:, None], max_score_indices] = solutions\n",
    "    return W, max_score_indices, solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(96, 256, device='cuda', requires_grad=True)\n",
    "y = torch.randn(32, 96, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.requires_grad = True\n",
    "y.requires_grad = False\n",
    "sets = run_omp(X, y, n_nonzero_coefs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.64592679,  1.41983684, -0.14846366, -1.13573509,  0.83928862,\n",
       "        1.06942006, -0.03079246, -0.00980617,  0.28853245,  0.19478957,\n",
       "       -0.30494662,  1.12142266,  1.37522674,  0.21715859,  0.2122062 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X, w = make_sparse_coded_signal(\n",
    "    n_samples=1,\n",
    "    n_components=256,\n",
    "    n_features=96,\n",
    "    n_nonzero_coefs=15,\n",
    "    random_state=4)\n",
    "\n",
    "y = torch.from_numpy(y).cuda()\n",
    "X = torch.from_numpy(X).cuda()\n",
    "y.shape, X.shape, w.shape\n",
    "w[w!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6459,  1.4198, -0.1485, -1.1357,  0.8393,  1.0694, -0.0308, -0.0098,\n",
       "         0.2885,  0.1948, -0.3049,  1.1214,  1.3752,  0.2172,  0.2122],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets = run_omp(X, y.T, n_nonzero_coefs=15)\n",
    "sets[0,:][sets[0,:]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, sets2, solutions2 = omp(X, y.T, n_nonzero_coefs=15)\n",
    "W[0,:][W[0,:]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6459,  1.4198, -0.1485, -1.1357,  0.8393,  1.0694, -0.0308, -0.0098,\n",
       "         0.2885,  0.1948, -0.3049,  1.1214,  1.3752,  0.2172,  0.2122],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.reshape(96, 1)\n",
    "W, sets2, solutions2 = omp(X, y.T, n_nonzero_coefs=15)\n",
    "W[0,:][W[0,:]!=0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "borealis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
