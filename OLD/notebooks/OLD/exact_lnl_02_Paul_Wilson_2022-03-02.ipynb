{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys \n",
    "import os \n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Args(): \n",
    "    ...\n",
    "\n",
    "args = Args()\n",
    "args.batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing cores: 100%|██████████| 528/528 [00:00<00:00, 87692.74it/s]\n",
      "Indexing Patches: 100%|██████████| 528/528 [00:00<00:00, 61144.50it/s]\n",
      "Preparing cores: 100%|██████████| 130/130 [00:00<00:00, 60456.76it/s]\n",
      "Indexing Patches: 100%|██████████| 130/130 [00:00<00:00, 57693.31it/s]\n",
      "Preparing cores: 100%|██████████| 138/138 [00:00<00:00, 89669.09it/s]\n",
      "Indexing Patches: 100%|██████████| 138/138 [00:00<00:00, 60735.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model...\n",
      "Creating the dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "print(\"Loading the dataset...\")\n",
    "from src.data.registry import (\n",
    "    exact_patches_sl_tuffc_prostate,\n",
    "    exact_patches_sl_tuffc_ndl,\n",
    ")\n",
    "\n",
    "train_dataset = exact_patches_sl_tuffc_prostate(\"train\")\n",
    "val_dataset = exact_patches_sl_tuffc_ndl(\"val\")\n",
    "test_dataset = exact_patches_sl_tuffc_ndl(\"test\")\n",
    "\n",
    "# load the model\n",
    "print(\"Loading the model...\")\n",
    "from src.modeling.registry import (\n",
    "    create_model,\n",
    "    vicreg_resnet10_pretrn_allcntrs_noPrst_ndl_crop,\n",
    ")\n",
    "\n",
    "backbone1 = vicreg_resnet10_pretrn_allcntrs_noPrst_ndl_crop(2).backbone\n",
    "fc1 = nn.Linear(512, 2)\n",
    "clf1 = nn.Sequential(backbone1, fc1)\n",
    "\n",
    "backbone2 = vicreg_resnet10_pretrn_allcntrs_noPrst_ndl_crop(2).backbone\n",
    "fc2 = nn.Linear(512, 2)\n",
    "\n",
    "# create the dataloaders\n",
    "print(\"Creating the dataloaders...\")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "labels = np.array(train_dataset.labels).astype(\"int\")\n",
    "weight_for_classes = [1 / sum(labels == label) for label in np.unique(labels)]\n",
    "weights = [weight_for_classes[label] for label in labels]\n",
    "# logging.info(f\"Weights for classes {weights}\")\n",
    "\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=weights,\n",
    "    num_samples=len(train_dataset),\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, sampler=train_sampler, num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.cuda()\n",
    "\n",
    "opt = torch.optim.Adam(fc1.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 955/3206 [01:25<03:20, 11.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/paulw/projects/TRUSnet-1/notebooks/exact_lnl_02_Paul_Wilson_2022-03-02.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.15.7.120/home/paulw/projects/TRUSnet-1/notebooks/exact_lnl_02_Paul_Wilson_2022-03-02.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.15.7.120/home/paulw/projects/TRUSnet-1/notebooks/exact_lnl_02_Paul_Wilson_2022-03-02.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     X, y, _ \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B130.15.7.120/home/paulw/projects/TRUSnet-1/notebooks/exact_lnl_02_Paul_Wilson_2022-03-02.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.15.7.120/home/paulw/projects/TRUSnet-1/notebooks/exact_lnl_02_Paul_Wilson_2022-03-02.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.15.7.120/home/paulw/projects/TRUSnet-1/notebooks/exact_lnl_02_Paul_Wilson_2022-03-02.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     opt\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def eval(model, loader):\n",
    "    from src.utils.accumulators import DictConcatenation\n",
    "    model.eval()\n",
    "    acc = DictConcatenation()\n",
    "    for batch in loader:\n",
    "        X, y, info = batch\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        y_hat = model(X).softmax(dim=1)\n",
    "        acc({\"y\": y, \"y_hat\": y_hat, **info})\n",
    "       \n",
    "    df = acc.compute(out_fmt=\"dataframe\")\n",
    "    metrics = {} \n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    metrics[\"patch_auc\"] = roc_auc_score(df.y, df.y_hat_1)\n",
    "    core_pred = df.groupby(\"core_specifier\").y_hat_1.mean()\n",
    "    core_label = df.groupby(\"core_specifier\").y.mean()\n",
    "    metrics[\"core_auc\"] = roc_auc_score(core_label, core_pred)\n",
    "\n",
    "    return metrics, df\n",
    "\n",
    "for batch in tqdm(train_loader):\n",
    "    X, y, _ = batch\n",
    "    X = X.cuda()\n",
    "    y = y.cuda()\n",
    "    opt.zero_grad()\n",
    "    y_hat = clf1(X)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "    loss.backward()\n",
    "    opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a5cbe58c68b89c0961f395f53ada90e2e240c2940fb46fd24937fe03ca7f49a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
